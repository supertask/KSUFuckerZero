学生ページ用クローラー
====
主に以下のURLを再帰的に辿っていき、クローリングして、データを収集します。

- http://www.cse.kyoto-su.ac.jp/~gX[学生証番号]/

実際のところは、一度クローリングした後に、高頻度に出現するページを再度クロールすることによって、できる限り多くのデータを入手することに成功しています。詳しいダウンロード元は、`./dlconfig.py`に記入されています。


## クローラーの実行方法
以下のコマンドを実行すると、`./dlconfig.py`のダウンロードコンフィグ情報を元にクローリングが行われます。

	$ python main.py <command>

コマンドには，4種類の簡易コマンドがあります．

- download\_all - 全ページをダウンロードする
- upload\_to\_s3 - AWS S3へデータを保存
- analyze\_HTMLs - 保存されたデータを形態素解析
- create\_index\_DB - キーワード検索用辞書の作成

ダウンロードする際，`./dlconfig.py`や`./Tool.py`でチューニングすることによって，無駄なデータの削減をすることができたり，より多くの重要なデータを見つけることが可能になりますが，詳しくは下のツールキットの説明を見てください．

また，ダウンロードされたデータは、`www.cse.kyoto-su.ac.jp/`のフォルダ内にダウンロードされ、 EC2内に情報が記録され，最終的にはS3へ保存されます．また，形態素解析した情報はRDSのMySQL内に保存されます．尚、クローリングにかかる時間は、およそ3~4時間（最適化はしていない）です．


## ツールキット（Tool.py）
Tool.pyは、以下のような役割があります。これは、オプションとして`main.py`にてダウンロードが終わった後に使うと、ストレージを節約したりするなどに役立ちます。

- 無駄なデータ（W3Cの検査結果など）を見つける
- 高頻度に出現するページ（課題ページなど）を見つける
- リンク先の多いスパムサイトを特定する（リンクが多すぎると検索に時間がかかるため）


## 解析する情報

- テキスト情報
	- HTMLをMeCabを使用して解析し、その人の学年、名前、特徴、リンクページなどを集めます。
- 画像情報
	- 今回は省いています..


## 解析データの保存先
`DB/`の中にそれぞれ解析したデータが保存されます。データベースの種類は以下の3つです。

- 推測した学生証番号 (MySQL内の`cse_students`テーブル)
- 解析した学生情報 (MySQL内の`cse_students`テーブル)
- 検索エンジンのためのキーワード辞書 (MySQL内の`keywords`テーブル)


## 動作環境
- メイン言語: Python
- フレームワーク:MeCab, MySQL
- コマンド: wget

